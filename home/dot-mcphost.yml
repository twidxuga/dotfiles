# MCPHost Configuration File
# All command-line flags can be configured here
# This demonstrates the simplified local/remote/builtin server configuration

# MCP Servers configuration
# Add your MCP servers here
# Examples for different server types:
mcpServers:
  # Local MCP servers - run commands locally via stdio transport
  # filesystem-local:
  #   type: "local"
  #   command: ["npx", "@modelcontextprotocol/server-filesystem", "/tmp"]
  #   environment:
  #     DEBUG: "true"
  #     LOG_LEVEL: "info"
  #
  #   sqlite:
  #     type: "local" 
  #     command: ["uvx", "mcp-server-sqlite", "--db-path", "/tmp/example.db"]
  #     environment:
  #       SQLITE_DEBUG: "1"
  #   
      
  # Builtin MCP servers - run in-process for optimal performance
  filesystem-builtin:
    type: "builtin"
    name: "fs"
    options:
      allowed_directories: ["/tmp", "/home/twidxuga/Desktop"]
      allowedTools: ["read_file", "write_file", "list_directory"]
    
  # Minimal builtin server - defaults to current working directory
  filesystem-cwd:
    type: "builtin"
    name: "fs"

  # Bash server for shell commands
  bash:
    type: "builtin"
    name: "bash"

  # Todo server for task management
  todo:
    type: "builtin"
    name: "todo"

  # Fetch server for web content
  fetch:
    type: "builtin"
    name: "fetch"

# Remote MCP servers - connect via StreamableHTTP transport
# Optional 'headers' field can be used for authentication and custom headers
    
# TODO
# websearch:
#   type: "remote"
#   url: "https://api.example.com/mcp"

#   websearch:
#     type: "remote"
#     url: "https://api.example.com/mcp"
#   
#   weather:
#     type: "remote"
#     url: "https://weather-mcp.example.com"
#   
#   # Legacy format still supported for backward compatibility:
#   # legacy-server:
#   #   command: npx
#   #   args: ["@modelcontextprotocol/server-filesystem", "/path"]
#   #   env:
#   #     MY_VAR: "value"

# Application settings (all optional)
# model: "anthropic:claude-sonnet-4-20250514"  # Default model to use
# max-steps: 10                                # Maximum agent steps (0 for unlimited)
# debug: false                                 # Enable debug logging
# system-prompt: "/path/to/system-prompt.txt" # System prompt text file

# Model generation parameters (all optional)
# max-tokens: 4096                             # Maximum tokens in response
# temperature: 0.7                             # Randomness (0.0-1.0)
# top-p: 0.95                                  # Nucleus sampling (0.0-1.0)
# top-k: 40                                    # Top K sampling
# stop-sequences: ["Human:", "Assistant:"]     # Custom stop sequences

# API Configuration (can also use environment variables)
# provider-api-key: "your-api-key"         # API key for OpenAI, Anthropic, or Google
# provider-url: "https://api.openai.com/v1" # Base URL for OpenAI, Anthropic, or Ollama
